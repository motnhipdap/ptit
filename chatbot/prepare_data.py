"""
Pipeline chu·∫©n b·ªã d·ªØ li·ªáu cho Chatbot T∆∞ v·∫•n Tuy·ªÉn sinh (RAG)
Quy tr√¨nh: Load Document -> Chunking -> Embedding -> L∆∞u MongoDB
Theo c·∫•u tr√∫c b√°o c√°o th·ª±c t·∫≠p:
  - chunk_size=800, chunk_overlap=400
  - MongoDB schema: { _id, content, embedding (Array 768) }
"""

# ==============================
# 1. C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN
# ==============================

#pip install langchain langchain-community langchain-huggingface sentence-transformers pymongo


from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from pymongo import MongoClient

# ==============================
# C·∫§U H√åNH MONGODB
# ==============================
MONGO_URI = "mongodb+srv://dungcony:vhErifXl4h1TwiMe@dungcony.qllqaij.mongodb.net/tuvantuyensinh?retryWrites=true&w=majority"
DB_NAME = "tuvantuyensinh"
COLLECTION_NAME = "documents"
VECTOR_INDEX_NAME = "vector_index"

# ==============================
# 2. LOAD DOCUMENT - T·∫£i vƒÉn b·∫£n 
# ==============================
print("=" * 50)
print("B∆Ø·ªöC 1: Load Document")
print("=" * 50)

# Load t·∫•t c·∫£ file .txt trong th∆∞ m·ª•c data/
loader_dir = DirectoryLoader("data/", glob="**/*.txt", loader_cls=TextLoader,
                              loader_kwargs={"encoding": "utf-8"})
documents = loader_dir.load()

print(f"S·ªë l∆∞·ª£ng t√†i li·ªáu ƒë√£ t·∫£i: {len(documents)}")
for i, doc in enumerate(documents):
    print(f"  - File {i+1}: {doc.metadata.get('source', 'N/A')} ({len(doc.page_content)} k√Ω t·ª±)")

# ==============================
# 3. CHUNKING - Chia nh·ªè vƒÉn b·∫£n
# ==============================
print("\n" + "=" * 50)
print("B∆Ø·ªöC 2: Chunking Document (chunk_size=800, overlap=400)")
print("=" * 50)

# Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè theo c·∫•u h√¨nh b√°o c√°o (H√¨nh 4.1.1):
# - chunk_size=800: m·ªói ƒëo·∫°n t·ªëi ƒëa 800 k√Ω t·ª±
# - chunk_overlap=400: tr√πng l·∫∑p 400 k√Ω t·ª± gi·ªØa c√°c ƒëo·∫°n li·ªÅn k·ªÅ
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=400
)
texts = text_splitter.split_documents(documents)

print(f"S·ªë l∆∞·ª£ng chunks sau khi chia: {len(texts)}")
for i, chunk in enumerate(texts[:5]):  # In 5 chunk ƒë·∫ßu ti√™n
    print(f"\n--- Chunk {i+1} ({len(chunk.page_content)} k√Ω t·ª±) ---")
    print(chunk.page_content[:150] + "...")
if len(texts) > 5:
    print(f"\n... v√† {len(texts) - 5} chunks kh√°c")

# ==============================
# 4. EMBEDDING MODEL
# ==============================
print("\n" + "=" * 50)
print("B∆Ø·ªöC 3: Kh·ªüi t·∫°o Embedding Model (768 chi·ªÅu)")
print("=" * 50)

# Model sentence-transformers cho embedding 768 chi·ªÅu (theo b√°o c√°o H√¨nh 4.1.2)
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

# Ki·ªÉm tra k√≠ch th∆∞·ªõc vector
test_vector = embedding_model.embed_query("test")
print(f"Embedding model: paraphrase-multilingual-MiniLM-L12-v2")
print(f"K√≠ch th∆∞·ªõc vector: {len(test_vector)} chi·ªÅu")

# ==============================
# 5. L∆ØU V√ÄO MONGODB
# ==============================
print("\n" + "=" * 50)
print("B∆Ø·ªöC 4: L∆∞u v√†o MongoDB")
print("=" * 50)

# K·∫øt n·ªëi MongoDB
client = MongoClient(MONGO_URI)
db = client[DB_NAME]
collection = db[COLLECTION_NAME]

# X√≥a d·ªØ li·ªáu c≈© (n·∫øu mu·ªën ch·∫°y l·∫°i t·ª´ ƒë·∫ßu)
old_count = collection.count_documents({})
if old_count > 0:
    print(f"X√≥a {old_count} b·∫£n ghi c≈©...")
    collection.delete_many({})

# T·∫°o embedding v√† l∆∞u t·ª´ng chunk v√†o MongoDB
# Schema theo b√°o c√°o (H√¨nh 4.1.2):
# { _id: ObjectId, content: String, embedding: Array(768) }
print(f"ƒêang t·∫°o embedding v√† l∆∞u {len(texts)} chunks v√†o MongoDB...")

mongo_docs = []
for i, chunk in enumerate(texts):
    # T·∫°o vector embedding cho ƒëo·∫°n vƒÉn b·∫£n
    vector = embedding_model.embed_query(chunk.page_content)

    # T·∫°o document theo schema b√°o c√°o
    mongo_doc = {
        "content": chunk.page_content,
        "embedding": vector
    }
    mongo_docs.append(mongo_doc)

    if (i + 1) % 10 == 0 or (i + 1) == len(texts):
        print(f"  ƒê√£ x·ª≠ l√Ω: {i+1}/{len(texts)} chunks")

# Bulk insert v√†o MongoDB
collection.insert_many(mongo_docs)
print(f"\n‚úÖ ƒê√£ l∆∞u {len(mongo_docs)} documents v√†o MongoDB")
print(f"   Database: {DB_NAME}")
print(f"   Collection: {COLLECTION_NAME}")
print(f"   Schema: {{ _id, content, embedding (Array {len(test_vector)}) }}")

# ==============================
# 6. T·∫†O VECTOR INDEX (H∆∞·ªõng d·∫´n)
# ==============================
print("\n" + "=" * 50)
print("B∆Ø·ªöC 5: T·∫°o Vector Search Index tr√™n MongoDB Atlas")
print("=" * 50)

print("""
‚ö†Ô∏è  B·∫°n c·∫ßn t·∫°o Vector Search Index tr√™n MongoDB Atlas:

1. Truy c·∫≠p MongoDB Atlas ‚Üí Database ‚Üí Collection ‚Üí Search Indexes
2. Ch·ªçn "Create Search Index" ‚Üí JSON Editor
3. ƒê·∫∑t t√™n index: "vector_index"
4. D√°n c·∫•u h√¨nh sau:

{
  "fields": [
    {
      "type": "vector",
      "path": "embedding",
      "numDimensions": 768,
      "similarity": "cosine"
    }
  ]
}

5. Nh·∫•n "Create Search Index"
""")

# ==============================
# 7. TEST T√åM KI·∫æM (Vector Search)
# ==============================
print("=" * 50)
print("B∆Ø·ªöC 6: Test Vector Search (theo H√¨nh 4.2.3)")
print("=" * 50)


def vector_search(query: str, num_candidates: int = 150, limit: int = 4):
    """
    T√¨m ki·∫øm vector trong MongoDB theo c·∫•u h√¨nh b√°o c√°o (H√¨nh 4.2.3):
    - index: "vector_index"
    - path: "embedding"
    - numCandidates: 150
    - limit: 4
    """
    query_vector = embedding_model.embed_query(query)

    pipeline = [
        {
            "$vectorSearch": {
                "index": VECTOR_INDEX_NAME,
                "path": "embedding",
                "queryVector": query_vector,
                "numCandidates": num_candidates,
                "limit": limit
            }
        },
        {
            "$project": {
                "_id": 0,
                "content": 1,
                "score": {"$meta": "vectorSearchScore"}
            }
        }
    ]

    results = list(collection.aggregate(pipeline))
    return results


# Th·ª≠ truy v·∫•n
queries = [
    "ƒêi·ªÉm chu·∫©n ng√†nh C√¥ng ngh·ªá th√¥ng tin l√† bao nhi√™u?",
    "H·ªçc ph√≠ tr∆∞·ªùng bao nhi√™u?",
    "C√≥ nh·ªØng ph∆∞∆°ng th·ª©c tuy·ªÉn sinh n√†o?",
]

try:
    for query in queries:
        print(f"\nüîç C√¢u h·ªèi: {query}")
        results = vector_search(query, num_candidates=150, limit=4)
        if results:
            for j, doc in enumerate(results):
                score = doc.get("score", 0)
                content = doc["content"][:120]
                print(f"  üìÑ K·∫øt qu·∫£ {j+1} (score: {score:.4f}): {content}...")
        else:
            print("  ‚ö†Ô∏è  Kh√¥ng c√≥ k·∫øt qu·∫£. H√£y ki·ªÉm tra Vector Search Index ƒë√£ ƒë∆∞·ª£c t·∫°o ch∆∞a.")
except Exception as e:
    print(f"\n‚ö†Ô∏è  Ch∆∞a th·ªÉ test vector search: {e}")
    print("   ‚Üí H√£y t·∫°o Vector Search Index tr√™n MongoDB Atlas tr∆∞·ªõc (xem h∆∞·ªõng d·∫´n ·ªü B∆∞·ªõc 5)")

print("\n‚úÖ Pipeline ho√†n t·∫•t! MongoDB ƒë√£ s·∫µn s√†ng cho chatbot RAG.")

# ƒê√≥ng k·∫øt n·ªëi
client.close()
